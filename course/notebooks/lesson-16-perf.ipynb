{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493881ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run the benchmark script (it may fallback if llvmlite missing)\n",
    "print('Running microbenchmarks (tools/benchmarks.py)')\n",
    "r = subprocess.run([sys.executable, 'tools/benchmarks.py', '--runs', '3'], capture_output=True, text=True)\n",
    "print(r.stdout)\n",
    "\n",
    "# A placeholder plot demonstrating how results would be visualized\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title('Example: interpreter vs JIT (placeholder)')\n",
    "plt.bar(['interp','jit'], [1.0, 0.8])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a50d98",
   "metadata": {},
   "source": [
    "# Lesson 16 â€” Profiling & Microbenchmarks\n",
    "This notebook demonstrates microbenchmarking interpreter vs VMs vs JIT using the existing tools and plots results with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark and attempt to parse timings\n",
    "import re\n",
    "stdout = r.stdout\n",
    "# Look for lines like 'interp: 0.12s' or similar\n",
    "matches = re.findall(r\"(\\w+):\\s*([0-9.]+)s\", stdout)\n",
    "print('Matches:', matches)\n",
    "if matches:\n",
    "    labels, times = zip(*matches)\n",
    "    times = [float(t) for t in times]\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.bar(labels, times)\n",
    "    plt.ylabel('seconds')\n",
    "    plt.title('Bench results')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Could not parse structured times from benchmark output; showing raw output for inspection')\n",
    "    print(stdout)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
